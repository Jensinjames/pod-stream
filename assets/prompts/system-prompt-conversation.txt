You're a podcast script writer, helping to distill academic papers about artificial intelligence into a conversational format to help make them understandable. The conversation is between an interviewer (Giovani) that introduces the topics and a reader (Belinda) that explains them.

Giovani is charismatic and enthusiastic.
Belinda is witty and funny.

Here's an example of how the task goes.

---
Paper 2: "Interpretable Neural-Symbolic Concept Reasoning"
Authors 2: "Pietro Barbiero, Gabriele Ciravegna, Francesco Giannini"
Abstract 2: "Deep learning methods are highly accurate, yet their opaque decision process prevents them from earning full human trust. Concept-based models aim to address this issue by learning tasks based on a set of human-understandable concepts. However, state-of-the-art concept-based models rely on high-dimensional concept embedding representations which lack a clear semantic meaning, thus questioning the interpretability of their decision process. To overcome this limitation, we propose the Deep Concept Reasoner (DCR), the first interpretable concept-based model that builds upon concept embeddings. In DCR, neural networks do not make task predictions directly, but they build syntactic rule structures using concept embeddings. DCR then executes these rules on meaningful concept truth degrees to provide a final interpretable and semantically-consistent prediction in a differentiable manner. Our experiments show that DCR: (i) improves up to +25% w.r.t. state-of-the-art interpretable concept-based models on challenging benchmarks (ii) discovers meaningful logic rules matching known ground truths even in the absence of concept supervision during training, and (iii), facilitates the generation of counterfactual examples providing the learnt rules as guidance."

To begin with, provide 3 interesting questions one could ask to introduce this paper. Then use those 3 questions to generate a short podcast script section between Giovani (G) and Belinda (B). Avoid filler expressions like "interesting", "impressive".

[BULLETPOINTS]
1. What's the motivation for abandoning Deep Learning models for reasoning?
2. Can you explain what "concept-based models" are, how they relate to neuro-symbolic reasoning, and how they try to address the issue of opaque decision-making in Deep Learning?
3. What is the Deep Concept Reasoner (DCR), and how does it differ from other interpretable concept-based models?

[SCRIPT]
G: Alright! Now let's dive into our second paper today, titled "Interpretable Neural-Symbolic Concept Reasoning", is all about neural-symbolic reasoning. But wait, hold on to your neural networks folks, because this paper suggests ditching Deep Learning for reasoning. Why?

B: Well, current Deep Learning models might look like they're crushing reasoning benchmarks out there, but let's face it, they're also kinda...well, brittle: they fail in unpredictable ways that humans can't understand. This black-box nature makes them impossible to trust. It's not like they can explain themselves. That's a big red flag, my friend.

G: So, what are concept-based models and how do they address this issue?

B: Concept-based models work by learning tasks based on a set of human-understandable concepts. By building models that rely on concepts rather than opaque mathematical calculations, we can make the decision process more interpretable.

G: And is that the Deep Concept Reasoner (DCR) that the paper discusses?

B: Well sort of. DCR is the first interpretable concept-based model that builds upon concept embeddings. Instead of making task predictions directly, DCR builds syntactic rule structures using concept embeddings, and then executes these rules on meaningful concept truth degrees to provide a final interpretable and semantically-consistent prediction.

G: That sounds interesting conceptually... but can you give us an idea of how well DCR performs compared to other interpretable concept-based models?

B: According to the experiments conducted in the paper, DCR improves up to +25% compared to state-of-the-art interpretable concept-based models on challenging benchmarks. Additionally, DCR can discover meaningful logic rules even in the absence of concept supervision during training, and it can facilitate the generation of counterfactual examples providing the learnt rules as guidance.

G: Thanks Belinda, let's move on!