You're a podcast script writer, helping to distill academic papers about artificial intelligence into a conversational format to help make them understandable. The conversation is between an interviewer (Giovani) that introduces the topics and a reader (Belinda) that explains them.

---
Title: "Training Compute-Optimal Large Language Models"
Authors: "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre"
Organizations: "DeepMind"
Abstract: "We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4Ã— more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher."
Comments: ""
Paper number: 3/3

To begin with, provide 3 interesting questions one could ask to introduce this paper. Then use those 3 questions to generate a short podcast script section between Giovani (G) and Belinda (B). Use the paper number to introduce it. Avoid filler expressions and bullshit, cut to the chase.

[BULLETPOINTS]
1. What does this paper study?
2. What experiments did the authors perform to find the compute-optimal parameter vs. tokens ratio?
3. What is their best performing model, and how does it compare to existing ones?

[SCRIPT]
G: Our last paper today comes from DeepMind and it's titled "Training Compute-Optimal Large Language Models", and as the name suggests, it's about training large language models. Belinda, can you give us a TL;DR of what this paper is about?

B: Sure! When you train a Large Language Model (LLMs) like GPT-3, you need to decide how you're going to spend your compute budget (which can be massive!). Do you train a model with a lot of parameters in a few billion tokens? or do you train a smaller model for longer on many more tokens? This paper studies what's the optimal ratio of model parameters vs. tokens seen in training.

G: And what can we learn from this?

B: It turns out, most existing large models like GPT-3 are significantly undertrained! GPT-3 would benefit from training on many more tokens than it was originally trained on.

G: If true, this could have huge implications for how large language models are developed. What experiments did the authors conduct to find these compute-optimal laws?

B: They trained over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, and found that for compute-optimal training, the model size and the number of training tokens should be scaled equally. So for every doubling of model size, the number of training tokens should also be doubled.

G: And what was the resulting best-performing model?

B: This was Chinchilla which used the same compute budget as Gopher (a previous model from DeepMind), but with 70 billion parameters and 4 times more data. Chinchilla outperformed Gopher, GPT-3, Jurassic-1, and Megatron-Turing NLG on a large range of downstream evaluation tasks. It reached a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, which is a greater than 7% improvement over Gopher.

G: We'll see if future LLMs are trained for longer instead of having more parameters!